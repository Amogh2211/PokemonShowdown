{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Evaluate the model\n",
    "    \n",
    "    print(\"Results against random player:\")\n",
    "    dqn.test(eval_env, nb_episodes = 100, verbose = False, visualize = False)\n",
    "    print(\n",
    "        f\"DQN Evalutation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    "    )\n",
    "    \n",
    "    second_opponent = MaxBasepowerPlayer(battle_format = \"gen8randombattle\")\n",
    "    eval_env.reset_env(restart = True, opponent = second_opponent)\n",
    "    print(\"Results against max base power:\")\n",
    "    dqn.test(eval_env, nb_episodes = 100, verbose = False, visualize = False)\n",
    "    print(\n",
    "        f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {env_env.n_finished_battles} episodes\")\n",
    "    \n",
    "    eval_env.reset_env(restart = False)\n",
    "    \n",
    "    # Evaluate player with included util method\n",
    "    n_challenges = 250\n",
    "    placement_battles = 40\n",
    "    eval_tast = background_evaluate_player(\n",
    "        eval_env.agent, n_challenges, placement_battles\n",
    "    )\n",
    "    dpn.test(eval_env, nb_episodes = n_challenges, verbose = False, visualize = False)\n",
    "    print(\"Evaluation with included method: \", eval_tast.result())\n",
    "    env_env.reset_env(restart = False)\n",
    "    \n",
    "    \n",
    "    # Cross evalutation\n",
    "    n_challenges = 50\n",
    "    players = [\n",
    "        eval_env.agent,\n",
    "        RandomPlayer(battle_format = \"gen8randombattle\"),\n",
    "        MaxBasePowerPlayer(battle_format = \"gen8randombattle\"),\n",
    "        SimpleHeuristicsPlayer(battle_format = \"gen8randombattle\"),\n",
    "    ]\n",
    "    \n",
    "    cross_eval_task = background_cross_evaluate(players, n_challenges)\n",
    "    dqn.test(\n",
    "    eval_env,\n",
    "    nb_episodes = n_challenges * (len(players) - 1),\n",
    "    verbpse = False,\n",
    "    visualize = False,\n",
    "    )\n",
    "    cross_evaluation = cross_eval_task.result()\n",
    "    table = [[\"-\"] + [p.username for p in players]]\n",
    "    for p_1, results in cross_evaluation.items():\n",
    "        table.append([p_1] + [cross_evaluation[p_1][p_2] for p_2 in results])\n",
    "    print(\"Cross eval with baselines: \")\n",
    "    print(tabulate(table))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
