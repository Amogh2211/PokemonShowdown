{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE NUMPY 1.21\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall --yes tensorflow\n",
    "#!pip uninstall --yes keras\n",
    "#!pip uninstall --yes keras-rl\n",
    "#!pip uninstall --yes keras-rl2\n",
    "#!pip install tensorflow==2.11.0\n",
    "#!pip install keras-rl2==1.0.4\n",
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.player import (\n",
    "    background_evaluate_player,\n",
    "    background_cross_evaluate,\n",
    "    Gen8EnvSinglePlayer,\n",
    "    RandomPlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObservationType,\n",
    ")  \n",
    "from poke_env.player.baselines import SimpleHeuristicsPlayer\n",
    "from poke_env import PlayerConfiguration\n",
    "from poke_env import ServerConfiguration\n",
    "from poke_env.player_configuration import PlayerConfiguration\n",
    "from poke_env.player.env_player import Gen8EnvSinglePlayer\n",
    "from poke_env.player.random_player import RandomPlayer\n",
    "from poke_env.player.player import Player\n",
    "from poke_env.server_configuration import LocalhostServerConfiguration\n",
    "\n",
    "myServerConfig = ServerConfiguration(\"my.custom.host:5432\", \"authentication-endpoint.com/action.php?\")\n",
    "\n",
    "myPlayerConfig = PlayerConfiguration(\"cr0nch\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from gym.spaces import Space, Box\n",
    "from gym.utils.env_checker import check_env\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "\n",
    "# myServerConfig = ServerConfiguration(\"my.custom.host:5432\", \"authentication-endpoint.com/action.php?\")\n",
    "\n",
    "# myPlayerConfig = PlayerConfiguration(\"cr0nch\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLPlayer(Gen8EnvSinglePlayer):\n",
    "    # Reward function\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value = 2.0, hp_value = 1.0, victory_value = 30.0\n",
    "        )\n",
    "    \n",
    "    # Damage multiplier and set up battle\n",
    "    \n",
    "    def embed_battle(self, battle: AbstractBattle)-> ObservationType:\n",
    "        moves_base_power = -np.ones(4)\n",
    "        multiplier = np.ones(4)\n",
    "        # Rescale for easier learning\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (move.base_power / 100) \n",
    "            if move.type:\n",
    "                multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                )\n",
    "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted])/6\n",
    "        fainted_mon_opponent = (len([mon for mon in battle.opponent_team.values() if mon.fainted])/6)\n",
    "        \n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                multiplier,\n",
    "                [fainted_mon_team, fainted_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "        return np.float32(final_vector)\n",
    "    \n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = [-1,-1,-1,-1,0,0,0,0,0,0]\n",
    "        high = [3,3,3,3,4,4,4,4,1,1]\n",
    "        return Box(\n",
    "            np.array(low, dtype = np.float32),\n",
    "            np.array(high, dtype = np.float32),\n",
    "            dtype = np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxDamagePlayer(RandomPlayer):\n",
    "    def choose_move(self, battle):\n",
    "        if battle.available_moves:\n",
    "            best_move = max(battle.available_moves, key = lambda move: move.base_power)\n",
    "            return self.create_order(best_move)\n",
    "        else:\n",
    "            return self.choose_random_move(battle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TRAINING_STEPS = 1\n",
    "NB_EVALUATION_EPISODES = 1\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Train DQN\n",
    "\n",
    "def dqn_training(player, dqn, nb_steps):\n",
    "    dqn.fit(player, nb_steps = nb_steps)\n",
    "    player.complete_current_battle()\n",
    "    \n",
    "def dqn_evaluation(player, dqn, nb_episodes):\n",
    "    player.reset_battles()\n",
    "    dqn.test(player, nb_episodes = nb_episodes, visualize = False, verbose = False)\n",
    "    \n",
    "    print(\n",
    "        \"DQN Evaluation: %d victories out of %d episodes\" % (player.n_won_battles, nb_episodes)\n",
    "    )\n",
    "\n",
    "async def final_tests():\n",
    "    await emb_player.send_challenges('cr0nch', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'opponent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-d5b017eb13f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     env_player = SimpleRLPlayer(\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mplayer_configuration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPlayerConfiguration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RL Player\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mbattle_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gen8randombattle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'opponent'"
     ]
    }
   ],
   "source": [
    "# Main function \n",
    "async def main():\n",
    "    \n",
    "    #test_env = SimpleRLPlayer(battle_format = \"gen9randombattle\", opponent = opponent, start_challenging = True)\n",
    "    #check_env(test_env)\n",
    "    #test_env.close()\n",
    "    \n",
    "    # Create one environment for training and one for evaluation\n",
    "    start = time.time()\n",
    "    \n",
    "    env_player = SimpleRLPlayer(\n",
    "        player_configuration = PlayerConfiguration(\"RL Player\", None),\n",
    "        battle_format = \"gen8randombattle\",\n",
    "        server_configuration = LocalhostServerConfiguration,\n",
    "    )\n",
    "    \n",
    "    opponent = RandomPlayer(\n",
    "        player_configuration = PlayerConfiguration(\"RandomPlayer\", None),\n",
    "        battle_format = \"gen8randombattle\",\n",
    "        server_configuration = LocalhostServerConfiguration,\n",
    "        )\n",
    "    \n",
    "    second_opponent = MaxDamagePlayer(\n",
    "        player_configuration = PlayerConfiguration(\"Max damage player\", None),\n",
    "        battle_format = \"gen8randombattle\",\n",
    "        server_configuration = LocalhostServerCofiguration,\n",
    "    )\n",
    "    \n",
    "    #test_env = SimpleRLPlayer(battle_format = \"gen8randombattle\")\n",
    "  # Dimensions \n",
    "    n_action = len(env_player.action_space)\n",
    "    input_shape = (1,10) # + train_env.observation_space.shape\n",
    "    \n",
    "    # Create Model\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation = \"elu\", input_shape = input_shape))\n",
    "    \n",
    "    # Our embedding have shape (1, 10), which affects our hidden layer\n",
    "    # dimension and output dimension\n",
    "    # Flattening resolve potential issues that would arise otherwis\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation = \"elu\"))\n",
    "    model.add(Dense(n_action, activation = \"linear\"))\n",
    "    \n",
    "    # Define DQN\n",
    "    memory = SequentialMemory(limit = 10000, window_length = 1)\n",
    "    \n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr = \"eps\",\n",
    "        value_max = 1.0,\n",
    "        value_min = 0.05,\n",
    "        value_test = 0.0,\n",
    "        nb_steps = 10000,\n",
    "    )\n",
    "    \n",
    "    dqn = DQNAgent(\n",
    "        model = model,\n",
    "        nb_actions = len(env_player.action_space),\n",
    "        policy = policy,\n",
    "        memory = memory,\n",
    "        nb_steps_warmup = 1000,\n",
    "        gamma = 0.5,\n",
    "        target_model_update = 1,\n",
    "        delta_clip = 0.01,\n",
    "        enable_double_dqn = True,\n",
    "    )\n",
    "    \n",
    "    dqn.compile(Adam(learning_rate = 0.0025), metrics = [\"mae\"])\n",
    "    \n",
    "    \n",
    "    class EmbeddedRLPlayer(Player):\n",
    "        def choose_move(self, battle):\n",
    "            if np.random.rand() < 0.01:\n",
    "                return self.choose_random_move(battle)\n",
    "            embedding = SimpleRLPlayer.embed_battle(self, battle)\n",
    "            action = dqn.forward(embedding)\n",
    "            return SimpleRLPlayer._action_to_move(self, action, battle)\n",
    "    \n",
    "    emb_player = EmbeddedRLPlayer(\n",
    "        env_algorithm = PlayerConfiguration (\"Embedded RL Player\", None),\n",
    "        battle_format = \"gen8randombattle\",\n",
    "        server_configuration = LocalhostServerConfiguration,\n",
    "    ) \n",
    "    \n",
    "    env_player.play_against(\n",
    "        env_algorithm = dqn_training,\n",
    "        opponent = opponent,\n",
    "        env_algorithm_kwargs = {\"dqn\": dqn, \"nb_steps\": NB_TRAINING_STEPS},\n",
    "    )\n",
    "    \n",
    "    model.save(\"model_%d\" % NB_TRAINING_STEPS)\n",
    "    \n",
    "    print(\"Results against random player:\")\n",
    "    env_player.play_against(\n",
    "        env_algorithm = dqn_evaluation,\n",
    "        opponent = opponent, \n",
    "        env_algorithm_kwargs = {\"dqn\": dqn, \"nb_steps\": NB_EVALUATION_STEPS},\n",
    "    )\n",
    "    \n",
    "    print(\"\\nResults against max player:\")\n",
    "    env_player.play_against(\n",
    "        env_algorithm=dqn_evaluation,\n",
    "        opponent=second_opponent,\n",
    "        env_algorithm_kwargs={\"dqn\": dqn, \"nb_episodes\": NB_EVALUATION_EPISODES},\n",
    "    )\n",
    "    asyncio.run(final_tests())\n",
    "    #eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    " #   asyncio.get_event_loop().run_until_complete(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
